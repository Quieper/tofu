{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import docx\n",
    "import openpyxl\n",
    "import time\n",
    "import os\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from operator import itemgetter\n",
    "from string import digits\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentimentScore(paragraph, weight=1):\n",
    "    sentences = []\n",
    "    lines_list = tokenize.sent_tokenize(paragraph)\n",
    "    sentences.extend(lines_list)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = 0\n",
    "    for sentence in sentences:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        score = ss['compound']\n",
    "        if score < 0:\n",
    "            sentiment += score*weight\n",
    "        else:\n",
    "            sentiment += score\n",
    "    return round(sentiment/len(sentences),2)\n",
    "\n",
    "def neg(bucket):\n",
    "    if bucket < 4:\n",
    "        return bucket + 4\n",
    "    else:\n",
    "        return bucket - 4\n",
    "    \n",
    "def remove_nan(l):\n",
    "    return [x for x in l if not pd.isnull(x)]\n",
    "\n",
    "def get_text(doc_names):\n",
    "    documents = []\n",
    "    titles = []\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    for i in range(len(doc_names)):\n",
    "        doc_name = doc_names[i]\n",
    "        paragraphs = docx.Document(doc_name).paragraphs\n",
    "        par_text = [paragraph.text.replace('\\xa0', ' ') for paragraph in paragraphs]\n",
    "        text = par_text[0]\n",
    "        titles.append(text)\n",
    "        for i in range(1, len(par_text)):\n",
    "            par = par_text[i]\n",
    "            if par:\n",
    "                text += ' ' + par\n",
    "        documents.append(text.lower().translate(remove_digits))\n",
    "    return titles, documents\n",
    "\n",
    "def bin_of_words(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    return [sorted(list(set(remove_nan(df[col].values)))) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of docs: 10\n",
      "\n",
      "Starting server with command: java -Xmx16G -cp /home/roguehydra/Documents/Jaar 3/TOFU/SA/stanford-corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-4b0c8d716ea84b9c.props -preload tokenize,ssplit,pos,depparse\n"
     ]
    },
    {
     "ename": "PermanentlyFailedException",
     "evalue": "Timed out waiting for service to come alive.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermanentlyFailedException\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4c137dd5cd3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# annotate documents on the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mann_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnegations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties_key, properties, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_OUTPUT_FORMAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# make the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stanfordnlp/server/client.py\u001b[0m in \u001b[0;36mensure_alive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPermanentlyFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timed out waiting for service to come alive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# At this point we are guaranteed that the service is alive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermanentlyFailedException\u001b[0m: Timed out waiting for service to come alive."
     ]
    }
   ],
   "source": [
    "CoreNLP = \"/home/roguehydra/Documents/Jaar 3/TOFU/SA/stanford-corenlp\"\n",
    "os.environ[\"CORENLP_HOME\"] = CoreNLP\n",
    "doc_names = sorted(glob.glob('/home/roguehydra/Documents/Jaar 4/TOFU/SA/Webpages/TEST/*.doc*'))\n",
    "titles, documents = get_text(doc_names)\n",
    "print(\"Amount of docs: {}\\n\".format(len(documents)))\n",
    "begin = time.time()\n",
    "ann_doc = []\n",
    "# set up the client\n",
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:\n",
    "    # annotate documents on the server\n",
    "    for text in documents:\n",
    "        ann_doc.append(client.annotate(text))\n",
    "        \n",
    "negations = []\n",
    "for ann in ann_doc:\n",
    "    negated=[]\n",
    "    token_dict = {}\n",
    "    for i in range(len(ann.sentence)):\n",
    "        # get the sentence\n",
    "        sentence = ann.sentence[i]\n",
    "\n",
    "        # get the dependency parse of the sentence\n",
    "        dependency_parse = sentence.basicDependencies\n",
    "\n",
    "        #get a dictionary associating each token/node with its label\n",
    "        for j in range(0, len(sentence.token)) :\n",
    "            token_dict[sentence.token[j].tokenEndIndex] = sentence.token[j].word\n",
    "\n",
    "        #take previous sentences into consideration\n",
    "        offset = sentence.token[0].tokenBeginIndex        \n",
    "\n",
    "        #get a list of the dependencies with the words they connect\n",
    "        for item in dependency_parse.edge:\n",
    "            dep = item.dep\n",
    "            if dep == 'neg':\n",
    "                source_node = item.source\n",
    "                source_name = token_dict[offset + source_node]\n",
    "                negated.append(source_name)\n",
    "    negations.append(negated)\n",
    "end = time.time()\n",
    "print(\"\\nElapsed time: {}s\".format(round(end - begin,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Dream \n",
    "\n",
    "2: Economic\n",
    "\n",
    "3: Health\n",
    "\n",
    "4: Environment\n",
    "\n",
    "5: Nightmare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_of_words = bin_of_words('BINS - version 2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = np.empty((len(documents), len(bins_of_words)), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    document = documents[i]\n",
    "    for j in range(len(bins_of_words)):\n",
    "        if str(word_matrix[i][j]) == 'None':\n",
    "            word_matrix[i][j] = []\n",
    "        bucket = bins_of_words[j]\n",
    "        for word in bucket:\n",
    "            if word in document:\n",
    "                for _ in range(document.count(word)):\n",
    "                    tmp = word_matrix[i][j].copy()\n",
    "                    tmp.append(word)\n",
    "                    word_matrix[i][j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [0,4]\n",
    "for i in range(len(negations)):\n",
    "    negated = negations[i]\n",
    "    for word in negated:\n",
    "        \n",
    "        for j in rows:\n",
    "            \n",
    "            if word in word_matrix[i][j]:\n",
    "                tmp = word_matrix[i][j].copy()\n",
    "                tmp.pop(tmp.index(word))\n",
    "                word_matrix[i][j] = tmp\n",
    "                \n",
    "                j2 = neg(j)\n",
    "                tmp2 = word_matrix[i][j2].copy()\n",
    "                tmp2.append(word)\n",
    "                word_matrix[i][j2] = tmp2 \n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix = np.zeros((len(documents), len(bins_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    for j in range(len(bins_of_words)):\n",
    "        score_matrix[i][j] = len(word_matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 \n",
      "\n",
      "Geothermal Basics\n",
      "\n",
      "Sentiment (score) : positive, (0.42)\n",
      "\n",
      "Economic score    : 0.36\n",
      "Health score      : 0.11\n",
      "Evironmental score: 0.53\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 2 \n",
      "\n",
      "Geothermal Energy Information and Facts\n",
      "\n",
      "Sentiment (score) : neutral (leaning towards positive), (0.09)\n",
      "\n",
      "Economic score    : 0.32\n",
      "Health score      : 0.12\n",
      "Evironmental score: 0.56\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 3 \n",
      "\n",
      "Heat Without Fire: Geothermal For A Cleaner, Sustainable Future In New York City\n",
      "\n",
      "Sentiment (score) : positive, (0.48)\n",
      "\n",
      "Economic score    : 0.27\n",
      "Health score      : 0.19\n",
      "Evironmental score: 0.54\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 4 \n",
      "\n",
      "Geothermal Heating and Cooling Technologies\n",
      "\n",
      "Sentiment (score) : positive, (0.3)\n",
      "\n",
      "Economic score    : 0.32\n",
      "Health score      : 0.18\n",
      "Evironmental score: 0.5\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 5 \n",
      "\n",
      "Geothermal for Canada – Questions and Challenges \n",
      "\n",
      "Sentiment (score) : neutral (leaning towards positive), (0.1)\n",
      "\n",
      "Economic score    : 0.34\n",
      "Health score      : 0.23\n",
      "Evironmental score: 0.43\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 6 \n",
      "\n",
      "How Geothermal Energy Works\n",
      "\n",
      "Sentiment (score) : positive, (0.3)\n",
      "\n",
      "Economic score    : 0.39\n",
      "Health score      : 0.17\n",
      "Evironmental score: 0.44\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 7 \n",
      "\n",
      "NEGATIVE ASPECTS OF GEOTHERMAL POWER GENERATION.\n",
      "\n",
      "Sentiment (score) : negative, (-0.46)\n",
      "\n",
      "Economic score    : 0.15\n",
      "Health score      : 0.21\n",
      "Evironmental score: 0.64\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 8 \n",
      "\n",
      "Pros and Cons of Geothermal Energy \n",
      "\n",
      "Sentiment (score) : positive, (0.18)\n",
      "\n",
      "Economic score    : 0.52\n",
      "Health score      : 0.09\n",
      "Evironmental score: 0.39\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 9 \n",
      "\n",
      "Renewable Energy\n",
      "\n",
      "Sentiment (score) : positive, (0.42)\n",
      "\n",
      "Economic score    : 0.27\n",
      "Health score      : 0.11\n",
      "Evironmental score: 0.62\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Document 10 \n",
      "\n",
      "Various Disadvantages of Geothermal Energy - Conserve Energy Future\n",
      "\n",
      "Sentiment (score) : positive, (0.18)\n",
      "\n",
      "Economic score    : 0.38\n",
      "Health score      : 0.2\n",
      "Evironmental score: 0.42\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    print(\"Document {} \\n\\n{}\\n\".format(i+1, titles[i]))\n",
    "    doc = documents[i]\n",
    "    vader_score = SentimentScore(doc,2)\n",
    "    positive = score_matrix[i][0]\n",
    "    negative = score_matrix[i][4]\n",
    "    total_sentiment = positive - negative\n",
    "    \n",
    "    economic = score_matrix[i][1] \n",
    "    health = score_matrix[i][2] \n",
    "    environment = score_matrix[i][3]\n",
    "    total_topic =  economic + health + environment\n",
    "    if total_sentiment > 0:\n",
    "        sen_score = total_sentiment/positive\n",
    "    elif total_sentiment < 0:\n",
    "        sen_score = total_sentiment/negative\n",
    "    elif total_sentiment == 0:\n",
    "        sen_score = 0\n",
    "        \n",
    "    score = (sen_score + vader_score) / 2\n",
    "    if score > 0.1:\n",
    "        judgement = 'positive'\n",
    "    elif score < -0.1:\n",
    "        judgement = 'negative'\n",
    "    elif score == 0:\n",
    "        judgement = 'neutral'\n",
    "    elif score > 0:\n",
    "        judgement = 'neutral (leaning towards positive)'\n",
    "    else:\n",
    "        judgement = 'neutral (leaning towards negative)'\n",
    "    \n",
    "    \n",
    "    print(\"Sentiment (score) : {}, ({})\\n\".format(judgement,round(score,2)))\n",
    "\n",
    "    print(\"Economic score    : {}\".format(round(economic/total_topic,2)))\n",
    "    print(\"Health score      : {}\".format(round(health/total_topic,2)))\n",
    "    print(\"Evironmental score: {}\".format(round(environment/total_topic,2)))\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, stop_words='english', ngram_range=(1,3), min_df=2)\n",
    "x = vectorizer.fit_transform(documents)\n",
    "tf_idf_scores = x.toarray()\n",
    "words = vectorizer.get_feature_names()\n",
    "all_words = [n for m in bins_of_words for n in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tf_idf_scores)):\n",
    "    print('Document {}\\n'.format(i+1))\n",
    "    document = tf_idf_scores[i]\n",
    "    sug = []\n",
    "    for j in range(len(document)):\n",
    "        score = document[j]\n",
    "        if score > 0:\n",
    "            word = words[j]\n",
    "            if word not in all_words:\n",
    "                sug.append((score, word))\n",
    "\n",
    "    if sug != []:\n",
    "        sug.sort(key=itemgetter(0), reverse=True)\n",
    "        for s, word in sug[-100:-90]:\n",
    "            print(word)\n",
    "    print('- - - - - - - - - - - - - - - -\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
