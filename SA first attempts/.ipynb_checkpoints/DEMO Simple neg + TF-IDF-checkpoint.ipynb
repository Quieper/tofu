{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import docx\n",
    "import openpyxl\n",
    "import time\n",
    "import os\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from operator import itemgetter\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg(bucket):\n",
    "    if bucket < 4:\n",
    "        return bucket + 4\n",
    "    else:\n",
    "        return bucket - 4\n",
    "    \n",
    "def remove_nan(l):\n",
    "    return [x for x in l if not pd.isnull(x)]\n",
    "\n",
    "def get_text(doc_names):\n",
    "    documents = []\n",
    "    titles = []\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    for i in range(len(doc_names)):\n",
    "        doc_name = doc_names[i]\n",
    "        paragraphs = docx.Document(doc_name).paragraphs\n",
    "        par_text = [paragraph.text.replace('\\xa0', ' ') for paragraph in paragraphs]\n",
    "        text = par_text[0]\n",
    "        titles.append(text)\n",
    "        for i in range(1, len(par_text)):\n",
    "            par = par_text[i]\n",
    "            if par:\n",
    "                text += ' ' + par\n",
    "        documents.append(text.lower().translate(remove_digits))\n",
    "    return titles, documents\n",
    "\n",
    "def bin_of_words(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    return [sorted(list(set(remove_nan(df[col].values)))) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of docs: 32\n",
      "\n",
      "Starting server with command: java -Xmx16G -cp /home/roguehydra/Documents/Jaar 3/TOFU/SA/stanford-corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-1b0cde8744a0494d.props -preload tokenize,ssplit,pos,depparse\n"
     ]
    }
   ],
   "source": [
    "CoreNLP = \"/home/roguehydra/Documents/Jaar 3/TOFU/SA/stanford-corenlp\"\n",
    "os.environ[\"CORENLP_HOME\"] = CoreNLP\n",
    "doc_names = sorted(glob.glob('/home/roguehydra/Documents/Jaar 3/TOFU/SA/Webpages/TRAIN/*.doc*'))\n",
    "titles, documents = get_text(doc_names)\n",
    "print(\"Amount of docs: {}\\n\".format(len(documents)))\n",
    "begin = time.time()\n",
    "ann_doc = []\n",
    "# set up the client\n",
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','depparse'], timeout=60000, memory='16G') as client:\n",
    "    # annotate documents on the server\n",
    "    for text in documents:\n",
    "        ann_doc.append(client.annotate(text))\n",
    "        \n",
    "negations = []\n",
    "for ann in ann_doc:\n",
    "    negated=[]\n",
    "    token_dict = {}\n",
    "    for i in range(len(ann.sentence)):\n",
    "        # get the sentence\n",
    "        sentence = ann.sentence[i]\n",
    "\n",
    "        # get the dependency parse of the sentence\n",
    "        dependency_parse = sentence.basicDependencies\n",
    "\n",
    "        #get a dictionary associating each token/node with its label\n",
    "        for j in range(0, len(sentence.token)) :\n",
    "            token_dict[sentence.token[j].tokenEndIndex] = sentence.token[j].word\n",
    "\n",
    "        #take previous sentences into consideration\n",
    "        offset = sentence.token[0].tokenBeginIndex        \n",
    "\n",
    "        #get a list of the dependencies with the words they connect\n",
    "        for item in dependency_parse.edge:\n",
    "            dep = item.dep\n",
    "            if dep == 'neg':\n",
    "                source_node = item.source\n",
    "                source_name = token_dict[offset + source_node]\n",
    "                negated.append(source_name)\n",
    "    negations.append(negated)\n",
    "end = time.time()\n",
    "print(\"\\nElapsed time: {}s\".format(round(end - begin,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Dream \n",
    "\n",
    "2: Economic\n",
    "\n",
    "3: Health\n",
    "\n",
    "4: Environment\n",
    "\n",
    "5: Nightmare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_of_words = bin_of_words('BINS - version 2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_matrix = np.empty((len(documents), len(bins_of_words)), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    document = documents[i]\n",
    "    for j in range(len(bins_of_words)):\n",
    "        if str(word_matrix[i][j]) == 'None':\n",
    "            word_matrix[i][j] = []\n",
    "        bucket = bins_of_words[j]\n",
    "        for word in bucket:\n",
    "            if word in document:\n",
    "                for _ in range(document.count(word)):\n",
    "                    tmp = word_matrix[i][j].copy()\n",
    "                    tmp.append(word)\n",
    "                    word_matrix[i][j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [0,4]\n",
    "for i in range(len(negations)):\n",
    "    negated = negations[i]\n",
    "    for word in negated:\n",
    "        \n",
    "        for j in rows:\n",
    "            \n",
    "            if word in word_matrix[i][j]:\n",
    "                tmp = word_matrix[i][j].copy()\n",
    "                tmp.pop(tmp.index(word))\n",
    "                word_matrix[i][j] = tmp\n",
    "                \n",
    "                j2 = neg(j)\n",
    "                tmp2 = word_matrix[i][j2].copy()\n",
    "                tmp2.append(word)\n",
    "                word_matrix[i][j2] = tmp2 \n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix = np.zeros((len(documents), len(bins_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    for j in range(len(bins_of_words)):\n",
    "        score_matrix[i][j] = len(word_matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    print(\"Document {} \\n\\n{}\\n\".format(i+1, titles[i]))\n",
    "    positive = score_matrix[i][0]\n",
    "    negative = score_matrix[i][4]\n",
    "    total_sentiment = positive - negative\n",
    "    \n",
    "    economic = score_matrix[i][1] \n",
    "    health = score_matrix[i][2] \n",
    "    environment = score_matrix[i][3]\n",
    "    total_topic =  economic + health + environment\n",
    "    if total_sentiment > 0:\n",
    "        sen_score = total_sentiment/positive\n",
    "        if sen_score > 0.1:\n",
    "            judgement = 'positive'\n",
    "        else:\n",
    "            judgement = 'neutral (leaning towards positive)'\n",
    "    elif total_sentiment < 0:\n",
    "        sen_score = total_sentiment/negative\n",
    "        if sen_score < -0.1:\n",
    "            judgement = 'negative'\n",
    "        else:\n",
    "            judgement = 'neutral (leaning towards negative)'\n",
    "    elif total_sentiment == 0:\n",
    "        judgement = 'neutral'\n",
    "        sen_score = 0\n",
    "    \n",
    "    print(\"Sentiment (score) : {}, ({})\\n\".format(judgement,round(sen_score,2)))\n",
    "    print(\"Economic score    : {}\".format(round(economic/total_topic,2)))\n",
    "    print(\"Health score      : {}\".format(round(health/total_topic,2)))\n",
    "    print(\"Evironmental score: {}\".format(round(environment/total_topic,2)))\n",
    "    print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, stop_words='english', ngram_range=(1,3))\n",
    "x = vectorizer.fit_transform(documents)\n",
    "tf_idf_scores = x.toarray()\n",
    "words = vectorizer.get_feature_names()\n",
    "all_words = [n for m in bins_of_words for n in m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tf_idf_scores)):\n",
    "    print('Document {}\\n'.format(i+1))\n",
    "    document = tf_idf_scores[i]\n",
    "    sug = []\n",
    "    for j in range(len(document)):\n",
    "        score = document[j]\n",
    "        if score > 0.2:\n",
    "            word = words[j]\n",
    "            if word not in all_words:\n",
    "                sug.append((score, word))\n",
    "\n",
    "    if sug != []:\n",
    "        sug.sort(key=itemgetter(0), reverse=True)\n",
    "        for s, word in sug:\n",
    "            print(word)\n",
    "    print('- - - - - - - - - - - - - - - -\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
